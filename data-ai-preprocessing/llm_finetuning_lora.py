# -*- coding: utf-8 -*-
"""llm-finetuning-lora.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1v1SyJ0IkeU6t3whiQZ83LPwV4UcLnAVb
"""

!pip install -q -U transformers datasets peft bitsandbytes accelerate trl scipy sentencepiece

!pip install "unsloth[colab-new] @ git+https://github.com/unslothai/unsloth.git"

from google.colab import files
import os

uploaded = files.upload()

from unsloth import FastLanguageModel
import torch

dtype = None
load_in_4bit = True

max_seq_length = 2048

!watch -n 1 nvidia-smi

model_name = "unsloth/Llama-3.2-3B-Instruct"

model, tokenizer = FastLanguageModel.from_pretrained(
    model_name=model_name,
    max_seq_length=max_seq_length,
    dtype=dtype,
    load_in_4bit=load_in_4bit,
)

from unsloth import FastLanguageModel

model = FastLanguageModel.get_peft_model(
    model,
    r=16,
    target_modules=[
        "q_proj", "k_proj", "v_proj", "o_proj",
        "gate_proj", "up_proj", "down_proj",
    ],
    lora_alpha=16,
    lora_dropout=0,
    bias="none",
    use_gradient_checkpointing="unsloth",
    random_state=3407,
    use_rslora=False,
    loftq_config=None,
)

from datasets import load_dataset

train_dataset = load_dataset('json', data_files='wuzzuf_train_llama.jsonl', split='train')
val_dataset = load_dataset('json', data_files='wuzzuf_validation_llama.jsonl', split='train')

train_dataset[0]

from datasets import load_dataset
from unsloth.chat_templates import get_chat_template

tokenizer = get_chat_template(
    tokenizer,
    chat_template = "llama-3",
    mapping = {"role" : "from", "content" : "value", "user" : "human", "assistant" : "gpt"},
)

def formatting_prompts_func(examples):
    instructions=examples["instruction"]
    inputs=examples["input"]
    outputs=examples["output"]
    texts=[]

    for instruction, input, output in zip(instructions, inputs, outputs):
        conversation = [
            {"from": "system", "value": instruction},
            {"from": "human", "value": input },
            {"from": "gpt", "value": output},
        ]
        text = tokenizer.apply_chat_template(conversation, tokenize=False, add_generation_prompt=False)
        texts.append(text)

    return { "text" : texts }

train_df = load_dataset('json', data_files='wuzzuf_train.jsonl', split='train')
train_df = train_df.map(formatting_prompts_func, batched = True)

shuffled_dataset = train_df.shuffle(seed=42)

sampled_pool = shuffled_dataset.select(range(800))

split_data = sampled_pool.train_test_split(test_size=0.1, seed=42)

train_dataset = split_data['train']
eval_dataset = split_data['test']

from trl import SFTTrainer
from transformers import TrainingArguments
from unsloth import FastLanguageModel

training_args = TrainingArguments(
    output_dir              = "./output",

    per_device_train_batch_size = 2,
    per_device_eval_batch_size  = 2,
    gradient_accumulation_steps = 4,

    num_train_epochs = 1,

    learning_rate = 2e-4,
    warmup_ratio  = 0.03,

    eval_strategy = "steps",
    eval_steps    = 20,
    save_strategy = "epoch",
    logging_steps = 5,

    optim = "adamw_8bit",
    weight_decay = 0.01,
    fp16 = not torch.cuda.is_bf16_supported(),
    bf16 = torch.cuda.is_bf16_supported(),

    report_to = "none",
    seed = 3407,
)

trainer = SFTTrainer(
    model = model,
    tokenizer = tokenizer,
    train_dataset = train_dataset,
    eval_dataset = eval_dataset,
    dataset_text_field = "text",
    max_seq_length = 2048,
    dataset_num_proc = 2,
    packing = False,
    args = training_args,
)

trainer_stats = trainer.train()

eval_results = trainer.evaluate()

print("\n" + "="*80)
print("üìà FINAL RESULTS")
print("="*80)
print(f"   Training loss: {trainer_stats.training_loss:.4f}")
print(f"   Validation loss: {eval_results['eval_loss']:.4f}")
print("="*80)

import json
training_info = {
    "model": model_name,
    "training_loss": float(trainer_stats.training_loss),
    "validation_loss": float(eval_results['eval_loss']),
    "training_time_minutes": 15 / 60,
    "epochs": training_args.num_train_epochs,
    "total_steps": trainer_stats.global_step,
    "lora_rank": 16,
    "effective_batch_size": training_args.per_device_train_batch_size * training_args.gradient_accumulation_steps,
}

with open("./lora_model_final/training_info.json", "w") as f:
    json.dump(training_info, f, indent=2)

FastLanguageModel.for_inference(model)

test_system_prompt = """You are an expert HR assistant specialized in creating professional job postings. When provided with minimal input (job title, career level, location, department, company, and optional key skills), your task is to generate two distinct, mandatory outputs:
1. A complete, engaging, and professional job posting in natural language format.
2. Structured JSON data for database storage.
You must follow this format precisely: generate the natural language posting first, followed by the unique separator: ---STRUCTURED-DATA--- and then the JSON output. The JSON must contain the following exact 10 keys: title, company, job_type, work_setting, location, experience_needed, career_level, education_level, job_categories, and skills. All ten fields must be completed, using the inferred value or 'N/A' if the data point is truly unknown. The value for the skills key must be a single, comma-separated string."""

conversation = [
    {"role": "system", "content": test_system_prompt},
    {"role": "user", "content": "Job Title: Quantum Computing Analyst\nCareer Level: Senior\nLocation: Berlin, Germany\nDepartment: R&D\nCompany: Qubit Solutions"},
    {"role": "assistant", "content": ""},
]

prompt_string = tokenizer.apply_chat_template(
    conversation,
    tokenize=False,
    add_generation_prompt=True
)

inputs = tokenizer(
    prompt_string,
    return_tensors="pt"
)

inputs = {k: v.to("cuda") for k, v in inputs.items()}

outputs = model.generate(
    **inputs,
    max_new_tokens=1500,
    temperature=0.7,
    top_p=0.9,
    do_sample=True,
    pad_token_id=tokenizer.eos_token_id,
)
text = tokenizer.decode(outputs[0])

print("\n--- Generated Output (Checklist) ---")
generated_text = text.split('<|start_header_id|>assistant<|end_header_id|>')[-1].strip()

print(generated_text)
print("-----------------------------------")

from huggingface_hub import login, HfApi

print("üîë Please login to Hugging Face:")
login()

repo_name = "farahelmashad/llama32-3b-finetuned-lora-new"
print(f"\nüì§ Creating repository: {repo_name}")

api = HfApi()
api.create_repo(repo_id=repo_name, private=False, exist_ok=True)

print(f"üì§ Uploading model files...")

api.upload_folder(
    folder_path="./lora_model_final",
    repo_id=repo_name,
    repo_type="model",
)

print(f"\n‚úÖ Model uploaded successfully!")
print(f"üîó View at: https://huggingface.co/{repo_name}")

repo_name = "farahelmashad/llama32-3b-job-posting-merged-new"
print("\nüì§ Uploading MERGED 16-bit model...")
try:
    model.push_to_hub_merged(
        repo_name,
        tokenizer,
        save_method="merged_16bit"
    )
    print(f"‚úÖ Merged 16-bit Model uploaded to: https://huggingface.co/{repo_name}")
except Exception as e:
    print(f"‚ùå Failed to upload merged 16-bit model. Try reducing the max memory usage. Error: {e}")

repo_name_gguf = repo_name + "-GGUF"
print(f"\nüì§ Uploading GGUF (Q4_K_M) model to a separate repo...")
try:
    model.push_to_hub_gguf(
        repo_name_gguf,

        tokenizer,
        quantization_method="q4_k_m"
    )
    print(f"‚úÖ GGUF Model uploaded to: https://huggingface.co/{repo_name_gguf}")
except Exception as e:
    print(f"‚ùå Failed to upload GGUF model. Error: {e}")

repo_name_lora = repo_name + "-LoRA-Adapter"
print("\nüì§ Uploading LoRA Adapters (original small files)...")
model.push_to_hub(repo_name_lora, tokenizer)
print(f"‚úÖ LoRA Adapters uploaded to: https://huggingface.co/{repo_name_lora}")

